{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d70551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73400a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c478471",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38079419",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "\n",
    "query_2 = queries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8753e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40c1236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51923caa",
   "metadata": {},
   "source": [
    "### WHY Divide by SQRT (DIMENSION)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb63ef",
   "metadata": {},
   "source": [
    "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.\n",
    "\n",
    "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94894f2b",
   "metadata": {},
   "source": [
    "### But Why SQRT?\n",
    "\n",
    "The dot product of  Q and K increases the variance because multiplying two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with the dimension. \n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af46baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 5.106730729143345\n",
      "Variance after scaling (dim=5): 1.0213461458286688\n",
      "Variance before scaling (dim=100): 103.50966809563538\n",
      "Variance after scaling (dim=100): 1.0350966809563538\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "        \n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    # Calculate variance of the dot products\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_100, variance_after_100 = compute_variance(100)\n",
    "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
    "print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13292711",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44324da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2 =torch.softmax(attn_scores_2/d_k**0.5 , dim=-1)\n",
    "\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95be4cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2 @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3abd0",
   "metadata": {},
   "source": [
    "## Implementing a Compact Self Attention Python Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "73efaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \n",
    "    def __init__(self,inputs):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.d_in = inputs.shape[1]\n",
    "        self.d_out = 2\n",
    "        self.W_query = nn.Parameter(torch.rand(self.d_in,self.d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(self.d_in,self.d_out))        \n",
    "        self.W_value = nn.Parameter(torch.rand(self.d_in,self.d_out))        \n",
    "\n",
    "    def forward(self):\n",
    "        keys = self.inputs @ self.W_key        \n",
    "        queries = self.inputs @ self.W_query\n",
    "        values = self.inputs @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5,dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights @ values\n",
    "        return context_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d39ee7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v1 = SelfAttention_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "114245fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sa_v1.forward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75ee9b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b2c1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self,inputs,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.d_in = inputs.shape[1]\n",
    "        self.d_out = 2\n",
    "        self.W_query = nn.Linear(self.d_in,self.d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(self.d_in,self.d_out,bias=qkv_bias)        \n",
    "        self.W_value = nn.Linear(self.d_in,self.d_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self):\n",
    "        keys = self.W_key(self.inputs)        \n",
    "        queries = self.W_query(self.inputs)\n",
    "        values = self.W_value(self.inputs)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5,dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights @ values\n",
    "        return context_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2969f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v1 = SelfAttention_v2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5339371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9907ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
