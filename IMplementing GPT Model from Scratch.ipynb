{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e34f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\":1024,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\":False\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e7ffaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])        \n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        #Use a placeholder for transformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "        *[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        #Use a Placeholder for layerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        X = tok_embeds + pos_embeds\n",
    "#         print(X.shape)\n",
    "        x = self.drop_emb(X)\n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "#         print(logits.shahen pe)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14f404dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0770, -0.5369, -0.6501,  ...,  0.4394,  0.3079,  1.2629],\n",
       "         [ 1.2624, -0.7648,  0.0168,  ..., -0.3997,  0.2210,  0.2918],\n",
       "         [ 0.4756,  0.0161, -1.4591,  ...,  1.1067,  0.0520, -1.1703],\n",
       "         [ 2.1591,  0.5228, -0.0922,  ...,  1.3586, -0.2436, -0.3894]],\n",
       "\n",
       "        [[-0.9236, -1.1660, -0.2661,  ...,  0.5079,  0.0575,  0.6692],\n",
       "         [ 0.3894, -0.9877, -0.4502,  ..., -0.4546,  1.0589, -0.2741],\n",
       "         [ 0.7883, -0.5361, -0.3320,  ...,  1.3431,  0.2862, -0.3871],\n",
       "         [ 1.1517,  0.9609,  0.8355,  ...,  1.8312, -0.6781,  0.2668]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= DummyGPTModel(GPT_CONFIG_124M)\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch,dim=0)\n",
    "x(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e772be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = nn.Embedding(50257,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d91c3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\AppData\\Local\\Temp\\ipykernel_17760\\88015324.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp(torch.tensor(batch))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8447,  0.9716, -1.1744,  ...,  0.6664,  0.4375,  0.0322],\n",
       "         [-0.7639, -0.0385, -0.9119,  ..., -0.0111,  0.5014, -0.1699],\n",
       "         [ 0.4867,  0.3748, -0.1322,  ...,  1.3440, -2.5451,  0.3944],\n",
       "         [ 0.3643,  0.3740,  0.4665,  ...,  2.4661, -0.5134, -0.2861]],\n",
       "\n",
       "        [[-0.8447,  0.9716, -1.1744,  ...,  0.6664,  0.4375,  0.0322],\n",
       "         [-1.2707, -1.7676, -0.1824,  ..., -0.2419, -0.8283,  0.7734],\n",
       "         [ 0.6793,  0.4326,  0.1375,  ..., -1.6505, -0.1433, -0.1632],\n",
       "         [ 1.9008,  0.7583,  0.7721,  ..., -0.3160, -1.2656,  0.4164]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(torch.tensor(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "445f0142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8bb85f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAyer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09200f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out = layer(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d174ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a954610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8719, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].mean()/out[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4bf7ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:  tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var = out.var(dim=-1,keepdim=True)\n",
    "print(\"Mean: \",mean)\n",
    "print(\"Variance: \",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "817710a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:  tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:  tensor([[-1.1921e-07],\n",
      "        [-3.7750e-07]], grad_fn=<MeanBackward1>)\n",
      "Var:  tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1,keepdim=True)\n",
    "var = out_norm.var(dim=-1,keepdim=True)\n",
    "print(\"Normalized layer outputs: \",out_norm)\n",
    "print(\"Mean: \",mean)\n",
    "print(\"Var: \",var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "90c1becd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9341e-09],\n",
       "        [0.0000e+00]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_norm.mean(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "72acb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        std = x.std(dim=-1,keepdim=True)        \n",
    "        norm_x = (x-mean)/(std + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e78b316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LayerNorm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "38c0143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8988, -1.0755,  0.8301, -1.1406,  0.2874, -1.0125,  0.5258,  1.7338,\n",
       "        -0.0569,  0.8072], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(torch.randn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9b978221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])        \n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        #Use a placeholder for transformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "        *[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        #Use a Placeholder for layerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        X = tok_embeds + pos_embeds\n",
    "#         print(X.shape)\n",
    "        x = self.drop_emb(X)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "#         print(logits.shahen pe)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6c849954",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DummyGPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3debf57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2204,  0.1094, -0.8397,  ..., -0.3039, -0.2998,  0.7156],\n",
       "         [-0.5685, -1.0897,  0.9066,  ..., -0.0817, -0.9036,  0.8595],\n",
       "         [-0.8813, -0.1679, -0.1316,  ..., -0.0539, -0.3489,  0.1407],\n",
       "         [-0.4067, -1.1791,  0.3187,  ...,  0.9100,  0.0186,  0.0661]],\n",
       "\n",
       "        [[-0.0232, -0.2497, -0.6211,  ..., -0.0812, -0.1655,  0.6932],\n",
       "         [-0.9943, -1.2283, -0.5148,  ...,  0.4684, -1.3461, -0.5234],\n",
       "         [ 0.6375, -0.2633, -0.0044,  ..., -0.2200,  0.6147,  0.7000],\n",
       "         [-0.4192,  0.0057,  0.1095,  ...,  0.0214,  0.1416,  0.0639]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8fbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
